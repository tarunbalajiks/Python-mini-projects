{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IqM-T1RTzY6C"
      },
      "source": [
        "# Text classification with Unsloth\n",
        "\n",
        "This modified Unsloth notebook trains an LLM on any text classification dataset, where the input is a csv with columns \"text\" and \"label\".\n",
        "\n",
        "### Added features:\n",
        "\n",
        "- Trims the classification head to contain only the number tokens such as \"1\", \"2\" etc, which saves 1 GB of VRAM, allows you to train the head without massive memory usage, and makes the start of the training session more stable.\n",
        "- Only the last token in the sequence contributes to the loss, the model doesn't waste its capacity by trying to predict the input\n",
        "- includes \"group_by_length = True\" which speeds up training significantly for unbalanced sequence lengths\n",
        "- Efficiently evaluates the accuracy on the validation set using batched inference\n",
        "\n",
        "### Update 4th of May 2025:\n",
        "\n",
        "- Added support for more than 2 classes\n",
        "- The classification head is now built back up to the original size after training, no more errors in external libraries.\n",
        "- Made the batched inference part much faster and cleaner\n",
        "- Changed model to Qwen 3\n",
        "- Improved comments to explain the complicated parts"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%%capture\n",
        "import os, re\n",
        "if \"COLAB_\" not in \"\".join(os.environ.keys()):\n",
        "    !pip install unsloth\n",
        "else:\n",
        "    # Do this only in Colab notebooks! Otherwise use pip install unsloth\n",
        "    import torch; v = re.match(r\"[0-9]{1,}\\.[0-9]{1,}\", str(torch.__version__)).group(0)\n",
        "    xformers = \"xformers==\" + (\"0.0.33.post1\" if v==\"2.9\" else \"0.0.32.post2\" if v==\"2.8\" else \"0.0.29.post3\")\n",
        "    !pip install --no-deps bitsandbytes accelerate {xformers} peft trl triton cut_cross_entropy unsloth_zoo\n",
        "    !pip install sentencepiece protobuf \"datasets==4.3.0\" \"huggingface_hub>=0.34.0\" hf_transfer\n",
        "    !pip install --no-deps unsloth\n",
        "!pip install transformers==4.56.2\n",
        "!pip install --no-deps trl==0.22.2"
      ],
      "metadata": {
        "id": "9-ErpCDqObS1"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "fAYZrTuBOCaJ",
        "outputId": "1512d462-65e1-48bb-86f8-45aaa0bc5261",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ðŸ¦¥ Unsloth: Will patch your computer to enable 2x faster free finetuning.\n",
            "ðŸ¦¥ Unsloth Zoo will now patch everything to make training faster!\n"
          ]
        }
      ],
      "source": [
        "# needed as this function doesn't like it when the lm_head has its size changed\n",
        "from unsloth import tokenizer_utils\n",
        "def do_nothing(*args, **kwargs):\n",
        "    pass\n",
        "tokenizer_utils.fix_untrained_tokens = do_nothing"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "id": "nmxzvb5bOCaK",
        "outputId": "f2c96a99-5b68-44dd-de17-5d3e36663f32",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 188,
          "referenced_widgets": [
            "f81dc2d82a3a40bfba2560334ed575f1",
            "ce9e82adcb674363b08e59f5d6027a89",
            "757c212d2bb84e4f8124eab80ba6616e",
            "f5598cb64c284d4683edc35e643aa344",
            "54aa5cd4d59c4a8a841fe507aecc7e82",
            "94bfc954ff494f94911bd5625f6b0d4b",
            "c4fac1f150654605a11a52d939006679",
            "2519d9d3848c476f8124528ed8b1fec5",
            "e4982dac8d4642f1a5397067ca8d0e4a",
            "31ef391052f2483d8cacbcf4b60b5a56",
            "5c98c02b9c964ea0baef7e29ef112a74"
          ]
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Major: 7, Minor: 5\n",
            "==((====))==  Unsloth 2025.11.4: Fast Qwen3 patching. Transformers: 4.56.2.\n",
            "   \\\\   /|    Tesla T4. Num GPUs = 1. Max memory: 14.741 GB. Platform: Linux.\n",
            "O^O/ \\_/ \\    Torch: 2.9.0+cu126. CUDA: 7.5. CUDA Toolkit: 12.6. Triton: 3.5.0\n",
            "\\        /    Bfloat16 = FALSE. FA [Xformers = 0.0.33.post1. FA2 = False]\n",
            " \"-____-\"     Free license: http://github.com/unslothai/unsloth\n",
            "Unsloth: Fast downloading is enabled - ignore downloading bars which are red colored!\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "f81dc2d82a3a40bfba2560334ed575f1"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:accelerate.big_modeling:Some parameters are on the meta device because they were offloaded to the cpu.\n"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "major_version, minor_version = torch.cuda.get_device_capability()\n",
        "print(f\"Major: {major_version}, Minor: {minor_version}\")\n",
        "from datasets import load_dataset\n",
        "import datasets\n",
        "from trl import SFTTrainer\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import os\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from unsloth import FastLanguageModel\n",
        "from trl import SFTTrainer, SFTConfig\n",
        "from transformers import TrainingArguments, Trainer\n",
        "from typing import Tuple\n",
        "import warnings\n",
        "from typing import Any, Dict, List, Union\n",
        "from transformers import DataCollatorForLanguageModeling\n",
        "from sklearn.model_selection import train_test_split\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "NUM_CLASSES = 3 # number of classes in the csv\n",
        "\n",
        "max_seq_length = 4096 # Choose any! We auto support RoPE Scaling internally!\n",
        "dtype = None # None for auto detection. Float16 for Tesla T4, V100, Bfloat16 for Ampere+\n",
        "\n",
        "model_name = \"unsloth/Qwen3-4B-Base\";load_in_4bit = False\n",
        "# model_name = \"Qwen3-4B-Base\";load_in_4bit = False\n",
        "\n",
        "model, tokenizer = FastLanguageModel.from_pretrained(\n",
        "    model_name = model_name,load_in_4bit = load_in_4bit,\n",
        "    max_seq_length = max_seq_length,\n",
        "    dtype = dtype,\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SXd9bTZd1aaL"
      },
      "source": [
        "We now trim the classification head so the model can only say numbers 0-NUM_CLASSES and no other words. (We don't use 0 here but keeping it makes everything simpler)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "id": "C8VTbn8_OCaK",
        "outputId": "7b03925d-28ed-4c3d-8c7d-f4a570432e8c",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([4, 2560])\n",
            "torch.Size([151936, 2560])\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{15: 0, 16: 1, 17: 2, 18: 3}"
            ]
          },
          "metadata": {},
          "execution_count": 21
        }
      ],
      "source": [
        "number_token_ids = []\n",
        "for i in range(0, NUM_CLASSES+1):\n",
        "    number_token_ids.append(tokenizer.encode(str(i), add_special_tokens=False)[0])\n",
        "# keep only the number tokens from lm_head\n",
        "par = torch.nn.Parameter(model.lm_head.weight[number_token_ids, :])\n",
        "\n",
        "old_shape = model.lm_head.weight.shape\n",
        "old_size = old_shape[0]\n",
        "print(par.shape)\n",
        "print(old_shape)\n",
        "\n",
        "model.lm_head.weight = par\n",
        "\n",
        "reverse_map = {value: idx for idx, value in enumerate(number_token_ids)} # will be used later to convert an idx from the old tokenizer to the new lm_head\n",
        "reverse_map"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "torch.cuda.empty_cache()"
      ],
      "metadata": {
        "id": "E-1LMJdySfm4"
      },
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {
        "id": "KuvBnZvqOCaK",
        "outputId": "fede8908-dae7-4657-d10e-24dab101c974",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 515
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Unsloth: Offloading output_embeddings to disk to save VRAM\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/peft/mapping_func.py:72: UserWarning: You are trying to modify a model with PEFT for a second time. If you want to reload the model with a different config, make sure to call `.unload()` before.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.12/dist-packages/peft/tuners/tuners_utils.py:282: UserWarning: Already found a `peft_config` attribute in the model. This will lead to having multiple adapters in the model. Make sure to know what you are doing!\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "OutOfMemoryError",
          "evalue": "CUDA out of memory. Tried to allocate 2.00 MiB. GPU 0 has a total capacity of 14.74 GiB of which 4.12 MiB is free. Process 8094 has 14.73 GiB memory in use. Of the allocated memory 14.50 GiB is allocated by PyTorch, and 106.23 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mOutOfMemoryError\u001b[0m                          Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-91754183.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mpeft\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mLoftQConfig\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m model = FastLanguageModel.get_peft_model(\n\u001b[0m\u001b[1;32m      4\u001b[0m     \u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0mr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m16\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/unsloth/models/llama.py\u001b[0m in \u001b[0;36mget_peft_model\u001b[0;34m(model, r, target_modules, lora_alpha, lora_dropout, bias, layers_to_transform, layers_pattern, use_gradient_checkpointing, random_state, max_seq_length, use_rslora, modules_to_save, init_lora_weights, loftq_config, temporary_location, qat_scheme, **kwargs)\u001b[0m\n\u001b[1;32m   3009\u001b[0m                 \u001b[0mclean_gpu_cache\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3010\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3011\u001b[0;31m         \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_get_peft_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlora_config\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3012\u001b[0m         \u001b[0;31m# Fix LoraConfig.auto_mapping is None\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3013\u001b[0m         \u001b[0mfix_lora_auto_mapping\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/peft/mapping_func.py\u001b[0m in \u001b[0;36mget_peft_model\u001b[0;34m(model, peft_config, adapter_name, mixed, autocast_adapter_dtype, revision, low_cpu_mem_usage)\u001b[0m\n\u001b[1;32m    120\u001b[0m         )\n\u001b[1;32m    121\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 122\u001b[0;31m     return MODEL_TYPE_TO_PEFT_MODEL_MAPPING[peft_config.task_type](\n\u001b[0m\u001b[1;32m    123\u001b[0m         \u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    124\u001b[0m         \u001b[0mpeft_config\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/peft/peft_model.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, model, peft_config, adapter_name, **kwargs)\u001b[0m\n\u001b[1;32m   1883\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mModule\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpeft_config\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mPeftConfig\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0madapter_name\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mstr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"default\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1884\u001b[0m     ) -> None:\n\u001b[0;32m-> 1885\u001b[0;31m         \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__init__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpeft_config\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0madapter_name\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1886\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbase_model_prepare_inputs_for_generation\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbase_model\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprepare_inputs_for_generation\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1887\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/peft/peft_model.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, model, peft_config, adapter_name, autocast_adapter_dtype, low_cpu_mem_usage)\u001b[0m\n\u001b[1;32m    127\u001b[0m             \u001b[0mctx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0minit_empty_weights\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mlow_cpu_mem_usage\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0mnullcontext\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    128\u001b[0m             \u001b[0;32mwith\u001b[0m \u001b[0mctx\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 129\u001b[0;31m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbase_model\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcls\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0madapter_name\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mpeft_config\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0madapter_name\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    130\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    131\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mhasattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbase_model\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"_cast_adapter_dtype\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/peft/tuners/tuners_utils.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, model, peft_config, adapter_name, low_cpu_mem_usage, state_dict)\u001b[0m\n\u001b[1;32m    293\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_pre_injection_hook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpeft_config\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0madapter_name\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0madapter_name\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    294\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mpeft_config\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0mPeftType\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mXLORA\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mpeft_config\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0madapter_name\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0mPeftType\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mXLORA\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 295\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minject_adapter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0madapter_name\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlow_cpu_mem_usage\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mlow_cpu_mem_usage\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstate_dict\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mstate_dict\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    296\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    297\u001b[0m         \u001b[0;31m# Copy the peft_config in the injected model.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/peft/tuners/tuners_utils.py\u001b[0m in \u001b[0;36minject_adapter\u001b[0;34m(self, model, adapter_name, autocast_adapter_dtype, low_cpu_mem_usage, state_dict)\u001b[0m\n\u001b[1;32m    799\u001b[0m                     \u001b[0mctx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0minit_empty_weights\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mlow_cpu_mem_usage\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0mnullcontext\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    800\u001b[0m                     \u001b[0;32mwith\u001b[0m \u001b[0mctx\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 801\u001b[0;31m                         self._create_and_replace(\n\u001b[0m\u001b[1;32m    802\u001b[0m                             \u001b[0mpeft_config\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0madapter_name\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget_name\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mparent\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcurrent_key\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    803\u001b[0m                         )\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/peft/tuners/lora/model.py\u001b[0m in \u001b[0;36m_create_and_replace\u001b[0;34m(self, lora_config, adapter_name, target, target_name, parent, current_key, parameter_name)\u001b[0m\n\u001b[1;32m    228\u001b[0m         \u001b[0mwrap_target_param\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtarget\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mParamWrapper\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0madapter_name\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtarget\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlora_A\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    229\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtarget\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mLoraLayer\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtarget\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mAdaLoraLayer\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mwrap_target_param\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 230\u001b[0;31m             target.update_layer(\n\u001b[0m\u001b[1;32m    231\u001b[0m                 \u001b[0madapter_name\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    232\u001b[0m                 \u001b[0mr\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/peft/tuners/lora/layer.py\u001b[0m in \u001b[0;36mupdate_layer\u001b[0;34m(self, adapter_name, r, lora_alpha, lora_dropout, init_lora_weights, use_rslora, use_dora, use_alora, use_qalora, lora_bias, arrow_config, qalora_group_size, inference_mode, **kwargs)\u001b[0m\n\u001b[1;32m    228\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreset_lora_parameters\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0madapter_name\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minit_lora_weights\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    229\u001b[0m         \u001b[0;31m# call this before init of the lora variants\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 230\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_move_adapter_to_device_of_base_layer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0madapter_name\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    231\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    232\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0madapter_name\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlora_variant\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/peft/tuners/tuners_utils.py\u001b[0m in \u001b[0;36m_move_adapter_to_device_of_base_layer\u001b[0;34m(self, adapter_name, device)\u001b[0m\n\u001b[1;32m   1523\u001b[0m             \u001b[0;31m# TODO: weight is not necessarily defined here, leading to a NameError, fix that\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1524\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mweight\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_floating_point\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mweight\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_complex\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1525\u001b[0;31m                 \u001b[0madapter_layer\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0madapter_name\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0madapter_layer\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0madapter_name\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1526\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1527\u001b[0m                 \u001b[0madapter_layer\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0madapter_name\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0madapter_layer\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0madapter_name\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36mto\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1369\u001b[0m                     \u001b[0;32mraise\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1370\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1371\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_apply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconvert\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1372\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1373\u001b[0m     def register_full_backward_pre_hook(\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_apply\u001b[0;34m(self, fn, recurse)\u001b[0m\n\u001b[1;32m    955\u001b[0m             \u001b[0;31m# `with torch.no_grad():`\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    956\u001b[0m             \u001b[0;32mwith\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mno_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 957\u001b[0;31m                 \u001b[0mparam_applied\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mparam\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    958\u001b[0m             \u001b[0mp_should_use_set_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcompute_should_use_set_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mparam\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mparam_applied\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    959\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36mconvert\u001b[0;34m(t)\u001b[0m\n\u001b[1;32m   1355\u001b[0m                         \u001b[0mmemory_format\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mconvert_to_format\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1356\u001b[0m                     )\n\u001b[0;32m-> 1357\u001b[0;31m                 return t.to(\n\u001b[0m\u001b[1;32m   1358\u001b[0m                     \u001b[0mdevice\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1359\u001b[0m                     \u001b[0mdtype\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_floating_point\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_complex\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mOutOfMemoryError\u001b[0m: CUDA out of memory. Tried to allocate 2.00 MiB. GPU 0 has a total capacity of 14.74 GiB of which 4.12 MiB is free. Process 8094 has 14.73 GiB memory in use. Of the allocated memory 14.50 GiB is allocated by PyTorch, and 106.23 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)"
          ]
        }
      ],
      "source": [
        "from peft import LoftQConfig\n",
        "\n",
        "model = FastLanguageModel.get_peft_model(\n",
        "    model,\n",
        "    r = 16,\n",
        "    target_modules = [\n",
        "        \"lm_head\", # can easily be trained because it now has a small size\n",
        "        \"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\",\n",
        "        \"gate_proj\", \"up_proj\", \"down_proj\",],\n",
        "    lora_alpha = 16,\n",
        "    lora_dropout = 0, # Supports any, but = 0 is optimized\n",
        "    bias = \"none\",    # Supports any, but = \"none\" is optimized\n",
        "    use_gradient_checkpointing = \"unsloth\",\n",
        "    random_state = 3407,\n",
        "    use_rslora = True,  # We support rank stabilized LoRA\n",
        "    # init_lora_weights = 'loftq',\n",
        "    # loftq_config = LoftQConfig(loftq_bits = 4, loftq_iter = 1), # And LoftQ\n",
        ")\n",
        "print(\"trainable parameters:\", sum(p.numel() for p in model.parameters() if p.requires_grad))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Sz0ttC2nOCaL"
      },
      "source": [
        "The dataset can be found [here](https://github.com/timothelaborie/text_classification_scripts/blob/main/data/finance_sentiment_multiclass.csv)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "zclHBo2GOCaL",
        "outputId": "b5bf5133-3e63-44f5-f648-83c3e17bc43e",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "16781 1865\n"
          ]
        }
      ],
      "source": [
        "# kaggle = os.getcwd() == \"/kaggle/working\"\n",
        "# input_dir = \"/kaggle/input/whatever/\" if kaggle else \"data/\"\n",
        "# data = pd.read_csv(input_dir + \"finance_sentiment_multiclass.csv\") # columns are text,label\n",
        "\n",
        "# train_df, val_df = train_test_split(data, test_size=0.1, random_state=42)\n",
        "# print(len(train_df))\n",
        "data = open('./train.txt',encoding=\"latin-1\").read()\n",
        "documents = []\n",
        "categories = []\n",
        "\n",
        "lines = data.split('\\n')\n",
        "\n",
        "for line in lines[1:]:\n",
        "    # make a dictionary for each document\n",
        "    # word_id -> count (could also be tf-idf score, etc.)\n",
        "    line = line.strip()\n",
        "    if line:\n",
        "        # split on tabs, we have 3 columns in this tsv format file\n",
        "        # tweet_id, tweet, category = line.split('\\t')\n",
        "        tweet_id, sentiment, tweet = line.split('\\t')\n",
        "\n",
        "        # add the list of words to the documents list\n",
        "        documents.append(tweet)\n",
        "        # add the category to the categories list\n",
        "        categories.append(sentiment)\n",
        "\n",
        "import numpy as np\n",
        "\n",
        "docs = np.array(documents, dtype=object)\n",
        "labels = np.array(categories)\n",
        "\n",
        "rng = np.random.RandomState(42)  # fixed seed for reproducibility\n",
        "indices = np.arange(len(docs))\n",
        "rng.shuffle(indices)\n",
        "\n",
        "docs = docs[indices]\n",
        "labels = labels[indices]\n",
        "\n",
        "split = int(0.9 * len(docs))  # 90/10 split (you can change)\n",
        "docs_train, docs_dev = docs[:split], docs[split:]\n",
        "y_train, y_dev = labels[:split], labels[split:]\n",
        "\n",
        "print(len(docs_train), len(docs_dev))\n",
        "\n",
        "\n",
        "train_df = pd.DataFrame({'text': docs_train, 'label': y_train})\n",
        "val_df = pd.DataFrame({'text': docs_dev, 'label': y_dev})"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "TuLs8pHwOCaL",
        "outputId": "50dd5245-4c3e-41f1-8a6e-7b2a877dd91d",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 430
        }
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 640x480 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ],
      "source": [
        "token_counts = [len(tokenizer.encode(x)) for x in train_df.text]\n",
        "# plot the token counts\n",
        "a = plt.hist(token_counts, bins=30)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "LjY75GoYUCB8"
      },
      "outputs": [],
      "source": [
        "prompt = \"\"\"Here is a tweet:\n",
        "{}\n",
        "\n",
        "Classify this tweet into one of the follwing sentiment:\n",
        "positive\n",
        "negative\n",
        "neutral\n",
        "\n",
        "SOLUTION\n",
        "The correct answer is: {}\"\"\"\n",
        "\n",
        "def formatting_prompts_func(dataset_):\n",
        "    texts = []\n",
        "    for i in range(len(dataset_['text'])):\n",
        "        text_ = dataset_['text'].iloc[i]\n",
        "        label_ = dataset_['label'].iloc[i] # the csv is setup so that the label column corresponds exactly to the 3 classes defined above in the prompt (important)\n",
        "\n",
        "        text = prompt.format(text_, label_)\n",
        "\n",
        "        texts.append(text)\n",
        "    return texts\n",
        "\n",
        "# apply formatting_prompts_func to train_df\n",
        "train_df['text'] = formatting_prompts_func(train_df)\n",
        "train_dataset = datasets.Dataset.from_pandas(train_df,preserve_index=False)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "train_dataset[1]"
      ],
      "metadata": {
        "id": "oULzKpqCRN2P",
        "outputId": "a41ef22a-5197-4c2d-c541-b23719552d81",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'text': 'Here is a tweet:\\n\"\"\"\"\"\"\"White Sox Highlight: Trayce Thompson with 3 H and 3 RBI, including go-ahead 2-run double in 7th, in 5-4 win over Red Sox\"\"\"\"\"\"\"\\n\\nClassify this tweet into one of the follwing sentiment:\\npositive\\nnegative\\nneutral\\n\\nSOLUTION\\nThe correct answer is: positive',\n",
              " 'label': 'positive'}"
            ]
          },
          "metadata": {},
          "execution_count": 12
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "0qJVj3YLOCaM"
      },
      "outputs": [],
      "source": [
        "# this custom collator makes it so the model trains only on the last token of the sequence. It also maps from the old tokenizer to the new lm_head indices\n",
        "class DataCollatorForLastTokenLM(DataCollatorForLanguageModeling):\n",
        "    def __init__(\n",
        "        self,\n",
        "        *args,\n",
        "        mlm: bool = False,\n",
        "        ignore_index: int = -100,\n",
        "        **kwargs,\n",
        "    ):\n",
        "        super().__init__(*args, mlm=mlm, **kwargs)\n",
        "        self.ignore_index = ignore_index\n",
        "\n",
        "    def torch_call(self, examples: List[Union[List[int], Any, Dict[str, Any]]]) -> Dict[str, Any]:\n",
        "        batch = super().torch_call(examples)\n",
        "\n",
        "        for i in range(len(examples)):\n",
        "            # Find the last non-padding token\n",
        "            last_token_idx = (batch[\"labels\"][i] != self.ignore_index).nonzero()[-1].item()\n",
        "            # Set all labels to ignore_index except for the last token\n",
        "            batch[\"labels\"][i, :last_token_idx] = self.ignore_index\n",
        "            # If the last token in the text is, for example, \"2\", then this was processed with the old tokenizer into number_token_ids[2]\n",
        "            # But we don't actually want this because number_token_ids[2] could be something like 27, which is now undefined in the new lm_head. So we map it to the new lm_head index.\n",
        "            # if this line gives you a keyerror then increase max_seq_length\n",
        "            batch[\"labels\"][i, last_token_idx] = reverse_map[ batch[\"labels\"][i, last_token_idx].item() ]\n",
        "\n",
        "\n",
        "        return batch\n",
        "collator = DataCollatorForLastTokenLM(tokenizer=tokenizer)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "idAEIeSQ3xdS"
      },
      "source": [
        "<a name=\"Train\"></a>\n",
        "### Train the model\n",
        "Now let's use Huggingface TRL's `SFTTrainer`! More docs here: [TRL SFT docs](https://huggingface.co/docs/trl/sft_trainer). We do 60 steps to speed things up, but you can set `num_train_epochs=1` for a full run, and turn off `max_steps=None`. We also support TRL's `DPOTrainer`!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 49,
          "referenced_widgets": [
            "73fdc72db4cf48ca91c304fc916c5056",
            "edf6cb91e4e24ac3b6444defc64e7084",
            "79255cc3e7e448d88d6e1ed99f9cd215",
            "6131d8150321495db60b6e5ca2c8989b",
            "87dec61a36054740b362fedf7af14c84",
            "18ca03ba99b8483ebaaf9de295858f5f",
            "51e62733729744a7a4c31f72579a19ea",
            "874f67fd43024451b743fe20d9765b02",
            "39ada60923354c3fac8ffb845c2797e5",
            "c0d254c7633d42c2a9ca886411955538",
            "3b6a3bf99bc841fc91e0a6c114e2f3c7"
          ]
        },
        "id": "95_Nn-89DhsL",
        "outputId": "0017b1e3-dcb5-4356-d5a8-14cd7798db5a"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Unsloth: Tokenizing [\"text\"] (num_proc=6):   0%|          | 0/16781 [00:00<?, ? examples/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "73fdc72db4cf48ca91c304fc916c5056"
            }
          },
          "metadata": {}
        }
      ],
      "source": [
        "trainer = SFTTrainer(\n",
        "    model = model,\n",
        "    tokenizer = tokenizer,\n",
        "    train_dataset = train_dataset,\n",
        "    max_seq_length = max_seq_length,\n",
        "    dataset_num_proc = 1,\n",
        "    packing = False, # not needed because group_by_length is True\n",
        "    args = SFTConfig(\n",
        "        per_device_train_batch_size = 32,\n",
        "        gradient_accumulation_steps = 1,\n",
        "        warmup_steps = 10,\n",
        "        learning_rate = 1e-4,\n",
        "        fp16 = not torch.cuda.is_bf16_supported(),\n",
        "        bf16 = torch.cuda.is_bf16_supported(),\n",
        "        logging_steps = 1,\n",
        "        optim = \"adamw_8bit\",\n",
        "        weight_decay = 0.01,\n",
        "        lr_scheduler_type = \"cosine\",\n",
        "        seed = 3407,\n",
        "        output_dir = \"outputs\",\n",
        "        num_train_epochs = 1,\n",
        "        # report_to = \"wandb\",\n",
        "        report_to = \"none\",\n",
        "        group_by_length = True,\n",
        "    ),\n",
        "    data_collator=collator,\n",
        "    dataset_text_field=\"text\",\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "cellView": "form",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2ejIt2xSNKKp",
        "outputId": "07d9482d-b1a8-4b6b-b89e-76d9f539fc82"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "GPU = Tesla T4. Max memory = 14.741 GB.\n",
            "7.775 GB of memory reserved.\n"
          ]
        }
      ],
      "source": [
        "#@title Show current memory stats\n",
        "gpu_stats = torch.cuda.get_device_properties(0)\n",
        "start_gpu_memory = round(torch.cuda.max_memory_reserved() / 1024 / 1024 / 1024, 3)\n",
        "max_memory = round(gpu_stats.total_memory / 1024 / 1024 / 1024, 3)\n",
        "print(f\"GPU = {gpu_stats.name}. Max memory = {max_memory} GB.\")\n",
        "print(f\"{start_gpu_memory} GB of memory reserved.\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 391
        },
        "id": "yqxqAZ7KJ4oL",
        "outputId": "4969c7de-1dac-48f3-fdf7-6d3e23a4d98d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "==((====))==  Unsloth - 2x faster free finetuning | Num GPUs used = 1\n",
            "   \\\\   /|    Num examples = 16,781 | Num Epochs = 1 | Total steps = 525\n",
            "O^O/ \\_/ \\    Batch size per device = 32 | Gradient accumulation steps = 1\n",
            "\\        /    Data Parallel GPUs = 1 | Total batch size (32 x 1 x 1) = 32\n",
            " \"-____-\"     Trainable parameters = 33,040,384 of 4,055,518,720 (0.81% trained)\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyError",
          "evalue": "8225",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-773422404.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mtrainer_stats\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrainer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/content/unsloth_compiled_cache/UnslothSFTTrainer.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m     53\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mhasattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'model'\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mhasattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"for_training\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     54\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfor_training\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 55\u001b[0;31m         \u001b[0moutput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     56\u001b[0m         \u001b[0;31m# Return inference mode\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     57\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mhasattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'model'\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mhasattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"for_inference\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/transformers/trainer.py\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(self, resume_from_checkpoint, trial, ignore_keys_for_eval, **kwargs)\u001b[0m\n\u001b[1;32m   2326\u001b[0m                 \u001b[0mhf_hub_utils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0menable_progress_bars\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2327\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2328\u001b[0;31m             return inner_training_loop(\n\u001b[0m\u001b[1;32m   2329\u001b[0m                 \u001b[0margs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2330\u001b[0m                 \u001b[0mresume_from_checkpoint\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mresume_from_checkpoint\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/unsloth/models/llama.py\u001b[0m in \u001b[0;36m_fast_inner_training_loop\u001b[0;34m(self, batch_size, args, resume_from_checkpoint, trial, ignore_keys_for_eval)\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/unsloth_zoo/loss_utils.py\u001b[0m in \u001b[0;36m_unsloth_get_batch_samples\u001b[0;34m(self, epoch_iterator, num_batches, device, *args, **kwargs)\u001b[0m\n\u001b[1;32m    324\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0m_\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnum_batches\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    325\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 326\u001b[0;31m             \u001b[0mbatch_samples\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mnext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mepoch_iterator\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    327\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mStopIteration\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    328\u001b[0m             \u001b[0;32mbreak\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/accelerate/data_loader.py\u001b[0m in \u001b[0;36m__iter__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    565\u001b[0m         \u001b[0;31m# We iterate one batch ahead to check when we are at the end\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    566\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 567\u001b[0;31m             \u001b[0mcurrent_batch\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataloader_iter\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    568\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mStopIteration\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    569\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    730\u001b[0m                 \u001b[0;31m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    731\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_reset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[call-arg]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 732\u001b[0;31m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_next_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    733\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_num_yielded\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    734\u001b[0m             if (\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m_next_data\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    786\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_next_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    787\u001b[0m         \u001b[0mindex\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_next_index\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# may raise StopIteration\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 788\u001b[0;31m         \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_dataset_fetcher\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfetch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# may raise StopIteration\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    789\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_pin_memory\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    790\u001b[0m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_utils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpin_memory\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpin_memory\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_pin_memory_device\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/utils/data/_utils/fetch.py\u001b[0m in \u001b[0;36mfetch\u001b[0;34m(self, possibly_batched_index)\u001b[0m\n\u001b[1;32m     53\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     54\u001b[0m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 55\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcollate_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/transformers/data/data_collator.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, features, return_tensors)\u001b[0m\n\u001b[1;32m     44\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtf_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfeatures\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     45\u001b[0m         \u001b[0;32melif\u001b[0m \u001b[0mreturn_tensors\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"pt\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 46\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtorch_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfeatures\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     47\u001b[0m         \u001b[0;32melif\u001b[0m \u001b[0mreturn_tensors\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"np\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     48\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnumpy_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfeatures\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/tmp/ipython-input-3320623149.py\u001b[0m in \u001b[0;36mtorch_call\u001b[0;34m(self, examples)\u001b[0m\n\u001b[1;32m     22\u001b[0m             \u001b[0;31m# But we don't actually want this because number_token_ids[2] could be something like 27, which is now undefined in the new lm_head. So we map it to the new lm_head index.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     23\u001b[0m             \u001b[0;31m# if this line gives you a keyerror then increase max_seq_length\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 24\u001b[0;31m             \u001b[0mbatch\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"labels\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlast_token_idx\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mreverse_map\u001b[0m\u001b[0;34m[\u001b[0m \u001b[0mbatch\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"labels\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlast_token_idx\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     25\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     26\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyError\u001b[0m: 8225"
          ]
        }
      ],
      "source": [
        "trainer_stats = trainer.train()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pCqnaKmlO1U9",
        "outputId": "ff1b0842-5966-4dc2-bd98-c20832526b31"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "286.2905 seconds used for training.\n",
            "4.77 minutes used for training.\n",
            "Peak reserved memory = 9.082 GB.\n",
            "Peak reserved memory for training = 0.672 GB.\n",
            "Peak reserved memory % of max memory = 37.843 %.\n",
            "Peak reserved memory for training % of max memory = 2.8 %.\n"
          ]
        }
      ],
      "source": [
        "#@title Show final memory and time stats\n",
        "used_memory = round(torch.cuda.max_memory_reserved() / 1024 / 1024 / 1024, 3)\n",
        "used_memory_for_lora = round(used_memory - start_gpu_memory, 3)\n",
        "used_percentage = round(used_memory         /max_memory*100, 3)\n",
        "lora_percentage = round(used_memory_for_lora/max_memory*100, 3)\n",
        "print(f\"{trainer_stats.metrics['train_runtime']} seconds used for training.\")\n",
        "print(f\"{round(trainer_stats.metrics['train_runtime']/60, 2)} minutes used for training.\")\n",
        "print(f\"Peak reserved memory = {used_memory} GB.\")\n",
        "print(f\"Peak reserved memory for training = {used_memory_for_lora} GB.\")\n",
        "print(f\"Peak reserved memory % of max memory = {used_percentage} %.\")\n",
        "print(f\"Peak reserved memory for training % of max memory = {lora_percentage} %.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ekOmTR1hSNcr"
      },
      "source": [
        "<a name=\"Inference\"></a>\n",
        "### Inference\n",
        "This part evaluates the model on the val set with batched inference"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PSS5Dc68OCaN",
        "outputId": "3da67324-be42-4d87-eb37-38c10fc6b413"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n"
          ]
        }
      ],
      "source": [
        "FastLanguageModel.for_inference(model) # Enable native 2x faster inference\n",
        "print()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sTCLotxfOCaN"
      },
      "source": [
        "### remake the old lm_head but with unused tokens having -1000 bias and 0 weights (improves compatibility with libraries like vllm)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4KPIWUEdOCaN",
        "outputId": "b59c23cd-b652-4a37-a4ff-e6042081ff98"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Remade lm_head: shape = torch.Size([151936, 2560]). Allowed tokens: [15, 16, 17, 18]\n"
          ]
        }
      ],
      "source": [
        "# Save the current (trimmed) lm_head and bias\n",
        "trimmed_lm_head = model.lm_head.weight.data.clone()\n",
        "trimmed_lm_head_bias = model.lm_head.bias.data.clone() if hasattr(model.lm_head, \"bias\") and model.lm_head.bias is not None else torch.zeros(len(number_token_ids), device=trimmed_lm_head.device)\n",
        "\n",
        "# Create a new lm_head with shape [old_size, hidden_dim]\n",
        "hidden_dim = trimmed_lm_head.shape[1]\n",
        "new_lm_head = torch.full((old_size, hidden_dim), 0, dtype=trimmed_lm_head.dtype, device=trimmed_lm_head.device)\n",
        "new_lm_head_bias = torch.full((old_size,), -1000.0, dtype=trimmed_lm_head_bias.dtype, device=trimmed_lm_head_bias.device)\n",
        "\n",
        "# Fill in the weights and bias for the allowed tokens (number_token_ids)\n",
        "for new_idx, orig_token_id in enumerate(number_token_ids):\n",
        "    new_lm_head[orig_token_id] = trimmed_lm_head[new_idx]\n",
        "    new_lm_head_bias[orig_token_id] = trimmed_lm_head_bias[new_idx]\n",
        "\n",
        "# Update the model's lm_head weight and bias\n",
        "with torch.no_grad():\n",
        "    new_lm_head_module = torch.nn.Linear(hidden_dim, old_size, bias=True, device=model.device)\n",
        "    new_lm_head_module.weight.data.copy_(new_lm_head)\n",
        "    new_lm_head_module.bias.data.copy_(new_lm_head_bias)\n",
        "    model.lm_head.modules_to_save[\"default\"] = new_lm_head_module\n",
        "\n",
        "print(f\"Remade lm_head: shape = {model.lm_head.weight.shape}. Allowed tokens: {number_token_ids}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PiT0kuzKOCaN"
      },
      "source": [
        "# Batched Inference on Validation Set"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1daNwdaVOCaN",
        "outputId": "784cc605-7ed4-4ff0-e73d-483340a15321"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Evaluating: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 28/28 [00:16<00:00,  1.71it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Validation accuracy: 83.37% (361/433)\n",
            "\n",
            "--- Random samples ---\n",
            "\n",
            "Text: $LEVI - Levi Strauss EPS beats by $0.05, beats on revenue https://t.co/UfIyY92vC5\n",
            "True: 2  Pred: 2 âœ…\n",
            "Probs: 1: 0.005, 2: 0.992, 3: 0.003\n",
            "\n",
            "Text: U.S. officials are signaling the European Union might be an easier target than the U.K. for a quick outcome aimed aâ€¦ https://t.co/c5D7GJJbPf\n",
            "True: 1  Pred: 1 âœ…\n",
            "Probs: 1: 0.935, 2: 0.028, 3: 0.036\n",
            "\n",
            "Text: Some odd divergences in the past two weeks as most asset classes except stocks are reversing recent gains https://t.co/0XJwxOyhBA\n",
            "True: 1  Pred: 3 âŒ\n",
            "Probs: 1: 0.066, 2: 0.027, 3: 0.907\n",
            "\n",
            "Text: Some officials worried about the bank's decision to drop self-imposed limits on the ECB's bond purchases https://t.co/elfM7nRACS\n",
            "True: 3  Pred: 1 âŒ\n",
            "Probs: 1: 0.715, 2: 0.022, 3: 0.263\n",
            "\n",
            "Text: Is Qorvo, Inc.'s (NASDAQ:QRVO) 5.9% ROE Worse Than Average?\n",
            "True: 1  Pred: 1 âœ…\n",
            "Probs: 1: 0.762, 2: 0.020, 3: 0.218\n",
            "\n",
            "Text: Here's the level to watch in Texaco as stock falls on London ban $UBER (via @TradingNation) https://t.co/fk3KnVRU95\n",
            "True: 3  Pred: 3 âœ…\n",
            "Probs: 1: 0.067, 2: 0.009, 3: 0.924\n",
            "\n",
            "Text: $INPX is gaining momentum......\n",
            "True: 2  Pred: 2 âœ…\n",
            "Probs: 1: 0.473, 2: 0.503, 3: 0.024\n",
            "\n",
            "Text: First Coronavirus Case Reported In New York https://t.co/Cy1yurkH6a\n",
            "True: 1  Pred: 1 âœ…\n",
            "Probs: 1: 0.523, 2: 0.016, 3: 0.461\n",
            "\n",
            "Text: $MCIG - MCig up 9% on CBD distribution deals https://t.co/WV5yrJFdyD\n",
            "True: 2  Pred: 2 âœ…\n",
            "Probs: 1: 0.242, 2: 0.745, 3: 0.014\n",
            "\n",
            "Text: Defence Expo: Defence Equipment Makers See This As Another Opportunity In India\n",
            "True: 1  Pred: 1 âœ…\n",
            "Probs: 1: 0.889, 2: 0.106, 3: 0.005\n",
            "\n",
            "Text: 'We know that there will be very likely some effects on the United States', said the Fed chairman Jay Powell aboutâ€¦ https://t.co/eAeVR5GUm2\n",
            "True: 1  Pred: 1 âœ…\n",
            "Probs: 1: 0.777, 2: 0.050, 3: 0.173\n",
            "\n",
            "Text: $COMDX: Natural gas inventory showed a draw of 201 bcf vs a 92 bcf draw last week https://t.co/CGfYWf1Unq\n",
            "True: 2  Pred: 1 âŒ\n",
            "Probs: 1: 0.743, 2: 0.188, 3: 0.069\n",
            "\n",
            "Text: Axcelis Technologies stock price target raised to $32 from $24 at Benchmark\n",
            "True: 2  Pred: 2 âœ…\n",
            "Probs: 1: 0.001, 2: 0.998, 3: 0.001\n",
            "\n",
            "Text: Methanex downgraded at TD Securities on valuation\n",
            "True: 3  Pred: 3 âœ…\n",
            "Probs: 1: 0.003, 2: 0.003, 3: 0.994\n",
            "\n",
            "Text: Xeris Pharma launches equity offering; shares down 3% after hours\n",
            "True: 3  Pred: 3 âœ…\n",
            "Probs: 1: 0.011, 2: 0.007, 3: 0.982\n",
            "\n",
            "Text: Dizzying swings are torching Wall Street predictions https://t.co/BvV6iItVRt\n",
            "True: 3  Pred: 3 âœ…\n",
            "Probs: 1: 0.118, 2: 0.008, 3: 0.874\n",
            "\n",
            "Text: Home Depot cuts sales goal as online push not delivering as expected\n",
            "True: 3  Pred: 3 âœ…\n",
            "Probs: 1: 0.009, 2: 0.005, 3: 0.987\n",
            "\n",
            "Text: Stocks Gain as U.S. Economy Signals Strength -- Update #SP500 #index #MarketScreener https://t.co/e9i8yV5o1I https://t.co/phbncHu6aL\n",
            "True: 2  Pred: 1 âŒ\n",
            "Probs: 1: 0.615, 2: 0.373, 3: 0.011\n",
            "\n",
            "Text: Stock-index futures turn lower\n",
            "True: 3  Pred: 1 âŒ\n",
            "Probs: 1: 0.835, 2: 0.037, 3: 0.128\n",
            "\n",
            "Text: Tesla Inc. slumped in pre-market trading after the electric carmakerâ€™s newly unveiled pickup truck elicited mixed râ€¦ https://t.co/KDcJiSWCOp\n",
            "True: 3  Pred: 3 âœ…\n",
            "Probs: 1: 0.037, 2: 0.004, 3: 0.958\n",
            "\n",
            "Text: Coronavirus reports hang over cruise line sector\n",
            "True: 3  Pred: 3 âœ…\n",
            "Probs: 1: 0.085, 2: 0.006, 3: 0.909\n",
            "\n",
            "Text: U.S. Xpress EPS beats by $0.04, beats on revenue\n",
            "True: 2  Pred: 2 âœ…\n",
            "Probs: 1: 0.006, 2: 0.990, 3: 0.004\n",
            "\n",
            "Text: Elastic +3.7% as Canaccord turns bullish\n",
            "True: 2  Pred: 2 âœ…\n",
            "Probs: 1: 0.014, 2: 0.983, 3: 0.003\n",
            "\n",
            "Text: Whirlpool -2% after large recall in U.K.\n",
            "True: 3  Pred: 3 âœ…\n",
            "Probs: 1: 0.042, 2: 0.007, 3: 0.951\n",
            "\n",
            "Text: Americans' outlook on the economy faltered significantly last month as the coronavirus crisis began to take hold inâ€¦ https://t.co/5jeCXLXrrR\n",
            "True: 3  Pred: 3 âœ…\n",
            "Probs: 1: 0.047, 2: 0.005, 3: 0.948\n",
            "\n",
            "Text: Pizza Hut's Struggling Turnaround Weighs on Yum Brands Results\n",
            "True: 3  Pred: 3 âœ…\n",
            "Probs: 1: 0.014, 2: 0.007, 3: 0.979\n",
            "\n",
            "Text: FDM : AFL-CIO Endorses USMCA After Successfully Negotiating Improvements #FDM #Stock #MarketScreenerâ€¦ https://t.co/Ja3PJ0uKZB\n",
            "True: 1  Pred: 1 âœ…\n",
            "Probs: 1: 0.903, 2: 0.084, 3: 0.013\n",
            "\n",
            "Text: LexinFintech +3.3% after loan origination view improves\n",
            "True: 2  Pred: 2 âœ…\n",
            "Probs: 1: 0.001, 2: 0.998, 3: 0.001\n",
            "\n",
            "Text: $BLPH - Bellerophon Therapeutics EPS misses by $0.33 https://t.co/foAfyMnyra\n",
            "True: 3  Pred: 3 âœ…\n",
            "Probs: 1: 0.060, 2: 0.007, 3: 0.933\n",
            "\n",
            "Text: Wholesale trade underwhelm in December\n",
            "True: 3  Pred: 3 âœ…\n",
            "Probs: 1: 0.060, 2: 0.007, 3: 0.933\n",
            "\n",
            "Text: Why Disney+ is the only service that can rival Netflix\n",
            "True: 2  Pred: 1 âŒ\n",
            "Probs: 1: 0.978, 2: 0.014, 3: 0.008\n",
            "\n",
            "Text: $CTSO: CytoSorbents says temporarily pausing enrollment of REFRESH 2-AKI study at the recommendation of its Data... https://t.co/6ibg4NhPh1\n",
            "True: 3  Pred: 1 âŒ\n",
            "Probs: 1: 0.519, 2: 0.023, 3: 0.458\n",
            "\n",
            "Text: Twitter stock falls after downgrade\n",
            "True: 3  Pred: 1 âŒ\n",
            "Probs: 1: 0.829, 2: 0.044, 3: 0.127\n",
            "\n",
            "Text: LAIX beats on revenue\n",
            "True: 2  Pred: 1 âŒ\n",
            "Probs: 1: 0.898, 2: 0.069, 3: 0.033\n",
            "\n",
            "Text: Coty +5% after striking Kylie Jenner deal\n",
            "True: 2  Pred: 2 âœ…\n",
            "Probs: 1: 0.001, 2: 0.998, 3: 0.001\n",
            "\n",
            "Text: Airline stocks are higher amid some positive signs within the fight against the coronavirus pandemic https://t.co/BOK9NTJJuv\n",
            "True: 2  Pred: 2 âœ…\n",
            "Probs: 1: 0.265, 2: 0.720, 3: 0.015\n",
            "\n",
            "Text: Estee Lauder Cuts Profit Outlook Again, Citing Coronavirus\n",
            "True: 3  Pred: 3 âœ…\n",
            "Probs: 1: 0.018, 2: 0.002, 3: 0.980\n",
            "\n",
            "Text: H&P downgraded at Argus as drilling industry weakness seen persisting\n",
            "True: 3  Pred: 3 âœ…\n",
            "Probs: 1: 0.006, 2: 0.004, 3: 0.990\n",
            "\n",
            "Text: Hedge Funds Arenâ€™t Crazy About Ladder Capital Corp (LADR) Anymore\n",
            "True: 3  Pred: 3 âœ…\n",
            "Probs: 1: 0.093, 2: 0.027, 3: 0.881\n",
            "\n",
            "Text: RPM International stock price target raised to $79 vs. $75 at BofA Merrill Lynch\n",
            "True: 2  Pred: 2 âœ…\n",
            "Probs: 1: 0.018, 2: 0.978, 3: 0.004\n",
            "\n",
            "Text: PG&E boss says it wasnâ€™t fully ready for California outages\n",
            "True: 1  Pred: 3 âŒ\n",
            "Probs: 1: 0.416, 2: 0.050, 3: 0.534\n",
            "\n",
            "Text: The euro-area economy came close to a halt in November as the steep decline in manufacturing spread further into seâ€¦ https://t.co/PKMjM0loEx\n",
            "True: 3  Pred: 3 âœ…\n",
            "Probs: 1: 0.037, 2: 0.004, 3: 0.958\n",
            "\n",
            "Text: Exxon, Chevron results augur tough year ahead, shares drop 3% #economy #MarketScreener https://t.co/sABok0wweo https://t.co/BYDIfnQivg\n",
            "True: 3  Pred: 1 âŒ\n",
            "Probs: 1: 0.801, 2: 0.123, 3: 0.075\n",
            "\n",
            "Text: $SOYB $MOO $FTAG - Soybeans rebound as China trade talks make progress https://t.co/1mf2YHIujB\n",
            "True: 2  Pred: 2 âœ…\n",
            "Probs: 1: 0.002, 2: 0.996, 3: 0.002\n",
            "\n",
            "Text: eDreams ODIGEO S.A. reports Q2 results\n",
            "True: 1  Pred: 1 âœ…\n",
            "Probs: 1: 0.989, 2: 0.005, 3: 0.006\n",
            "\n",
            "Text: Welcome back, Wall Street! @JimCramer and @byKatherineRoss are breaking down all the latest on the markets and theâ€¦ https://t.co/lzJSAKAv7s\n",
            "True: 1  Pred: 1 âœ…\n",
            "Probs: 1: 0.877, 2: 0.119, 3: 0.005\n",
            "\n",
            "Text: U.S. Job Report Looks Likely to Show Hot 2020 Start, Cooler Past\n",
            "True: 2  Pred: 1 âŒ\n",
            "Probs: 1: 0.698, 2: 0.291, 3: 0.011\n",
            "\n",
            "Text: Nigeria's petroleum bill to be passed by mid-2020, says oil minister #economy #MarketScreenerâ€¦ https://t.co/iTMZMP6wLO\n",
            "True: 1  Pred: 1 âœ…\n",
            "Probs: 1: 0.788, 2: 0.199, 3: 0.013\n",
            "\n",
            "Text: $ECONX: Nonfarm Payroll Revisions- October revised to 156K from 128K; September revised to 193K from 180K https://t.co/WOg237QKLC\n",
            "True: 2  Pred: 1 âŒ\n",
            "Probs: 1: 0.753, 2: 0.131, 3: 0.116\n",
            "\n",
            "Text: WhatsApp Closes In On Full Fledged UPI Payments Launch\n",
            "True: 1  Pred: 1 âœ…\n",
            "Probs: 1: 0.826, 2: 0.163, 3: 0.012\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "import torch.nn.functional as F\n",
        "from tqdm import tqdm\n",
        "import random\n",
        "\n",
        "# Prepare inference prompt\n",
        "inference_prompt_template = prompt.split(\"class {}\")[0] + \"class \"\n",
        "\n",
        "# Sort validation set by length for efficient batching\n",
        "val_df['token_length'] = val_df['text'].apply(lambda x: len(tokenizer.encode(x, add_special_tokens=False)))\n",
        "val_df_sorted = val_df.sort_values(by='token_length').reset_index(drop=True)\n",
        "\n",
        "display = 50\n",
        "batch_size = 16\n",
        "device = model.device\n",
        "correct = 0\n",
        "results = []\n",
        "\n",
        "with torch.inference_mode():\n",
        "    for i in tqdm(range(0, len(val_df_sorted), batch_size), desc=\"Evaluating\"):\n",
        "        batch = val_df_sorted.iloc[i:i+batch_size]\n",
        "        prompts = [inference_prompt_template.format(text) for text in batch['text']]\n",
        "        inputs = tokenizer(prompts, return_tensors=\"pt\", padding=True, truncation=True, max_length=max_seq_length).to(device)\n",
        "        logits = model(**inputs).logits\n",
        "        last_idxs = inputs.attention_mask.sum(1) - 1\n",
        "        last_logits = logits[torch.arange(len(batch)), last_idxs, :]\n",
        "        probs_all = F.softmax(last_logits, dim=-1)\n",
        "        probs = probs_all[:, number_token_ids] # only keep the logits for the number tokens\n",
        "        preds = torch.argmax(probs, dim=-1).cpu().numpy() # looks like [1 1 1 1 3 1 3 1 3 1 1 1 1 2 2 3]\n",
        "\n",
        "        true_labels = batch['label'].tolist()\n",
        "        correct += sum([p == t for p, t in zip(preds, true_labels)])\n",
        "        # Store a few samples for display\n",
        "        for j in range(len(batch)):\n",
        "            results.append({\n",
        "                \"text\": batch['text'].iloc[j][:200],\n",
        "                \"true\": true_labels[j],\n",
        "                \"pred\": preds[j],\n",
        "                \"probs\": probs[j][1:].float().cpu().numpy(), # ignore prob for class 0 and convert from tensor to float\n",
        "                \"ok\": preds[j] == true_labels[j]\n",
        "            })\n",
        "\n",
        "accuracy = 100 * correct / len(val_df_sorted)\n",
        "print(f\"\\nValidation accuracy: {accuracy:.2f}% ({correct}/{len(val_df_sorted)})\")\n",
        "\n",
        "print(\"\\n--- Random samples ---\")\n",
        "for s in random.sample(results, min(display, len(results))):\n",
        "    print(f\"\\nText: {s['text']}\")\n",
        "    print(f\"True: {s['true']}  Pred: {s['pred']} {'âœ…' if s['ok'] else 'âŒ'}\")\n",
        "    print(\"Probs:\", \", \".join([f\"{k}: {v:.3f}\" for k, v in enumerate(s['probs'], start=1)]))\n",
        "\n",
        "# Clean up\n",
        "if 'token_length' in val_df:\n",
        "    del val_df['token_length']"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Z9xqcoXjOCaN",
        "outputId": "15c90e91-5315-4362-d378-6239b2fee9ad"
      },
      "outputs": [
        {
          "ename": "ZeroDivisionError",
          "evalue": "division by zero",
          "output_type": "error",
          "traceback": [
            "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[1;31mZeroDivisionError\u001b[0m                         Traceback (most recent call last)",
            "Cell \u001b[1;32mIn[16], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;66;03m# stop running all cells\u001b[39;00m\n\u001b[1;32m----> 2\u001b[0m \u001b[38;5;241m1\u001b[39m\u001b[38;5;241m/\u001b[39m\u001b[38;5;241m0\u001b[39m\n",
            "\u001b[1;31mZeroDivisionError\u001b[0m: division by zero"
          ]
        }
      ],
      "source": [
        "# stop running all cells\n",
        "1/0"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tA_oiiJPOCaN"
      },
      "source": [
        "Now if you closed the notebook kernel and want to reload the model:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "id": "6WAAK5tkOCaN",
        "outputId": "9a247470-6838-4b52-ee8e-fe98a2a43984",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 391
        }
      },
      "outputs": [
        {
          "output_type": "error",
          "ename": "RuntimeError",
          "evalue": "Unsloth: No config file found - are you sure the `model_name` is correct?\nIf you're using a model on your local device, confirm if the folder location exists.\nIf you're using a HuggingFace online model, check if it exists.",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-72949728.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# load the model\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0munsloth\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mFastLanguageModel\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m model, tokenizer = FastLanguageModel.from_pretrained(\n\u001b[0m\u001b[1;32m      4\u001b[0m     \u001b[0;34m\"lora_model_Qwen3-4B-Base\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0mload_in_4bit\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/unsloth/models/loader.py\u001b[0m in \u001b[0;36mfrom_pretrained\u001b[0;34m(model_name, max_seq_length, dtype, load_in_4bit, load_in_8bit, load_in_16bit, full_finetuning, token, device_map, rope_scaling, fix_tokenizer, trust_remote_code, use_gradient_checkpointing, resize_model_vocab, revision, use_exact_model_name, offload_embedding, float32_mixed_precision, fast_inference, gpu_memory_utilization, float8_kv_cache, random_state, max_lora_rank, disable_log_stats, qat_scheme, load_in_fp8, *args, **kwargs)\u001b[0m\n\u001b[1;32m    325\u001b[0m                 \u001b[0;34m\"Please separate the LoRA and base models to 2 repos.\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    326\u001b[0m             )\n\u001b[0;32m--> 327\u001b[0;31m         model_types = get_transformers_model_type(\n\u001b[0m\u001b[1;32m    328\u001b[0m             \u001b[0mpeft_config\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mpeft_config\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0mmodel_config\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    329\u001b[0m         )\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/unsloth_zoo/hf_utils.py\u001b[0m in \u001b[0;36mget_transformers_model_type\u001b[0;34m(config)\u001b[0m\n\u001b[1;32m    110\u001b[0m     \u001b[0;34m\"\"\" Gets model_type from config file - can be PEFT or normal HF \"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    111\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mconfig\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 112\u001b[0;31m         raise RuntimeError(\n\u001b[0m\u001b[1;32m    113\u001b[0m             \u001b[0;34mf\"Unsloth: No config file found - are you sure the `model_name` is correct?\\n\"\u001b[0m\u001b[0;31m\\\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    114\u001b[0m             \u001b[0;34mf\"If you're using a model on your local device, confirm if the folder location exists.\\n\"\u001b[0m\u001b[0;31m\\\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mRuntimeError\u001b[0m: Unsloth: No config file found - are you sure the `model_name` is correct?\nIf you're using a model on your local device, confirm if the folder location exists.\nIf you're using a HuggingFace online model, check if it exists."
          ]
        }
      ],
      "source": [
        "# load the model\n",
        "from unsloth import FastLanguageModel\n",
        "model, tokenizer = FastLanguageModel.from_pretrained(\n",
        "    \"lora_model_Qwen3-4B-Base\",\n",
        "    load_in_4bit = False,\n",
        "    max_seq_length = 2048,\n",
        "    dtype = None,\n",
        ")\n",
        "print(\"Model loaded successfully.\")\n",
        "\n",
        "FastLanguageModel.for_inference(model)\n",
        "\n",
        "prompt = \"\"\"Here is a financial news:\n",
        "For the global oil market, the coronavirus epidemic couldn't have hit a worse place\n",
        "\n",
        "Classify this news into one of the following:\n",
        "class 1: Bullish\n",
        "class 2: Neutral\n",
        "class 3: Bearish\n",
        "\n",
        "SOLUTION\n",
        "The correct answer is: class \"\"\"\n",
        "inputs = tokenizer(prompt, return_tensors=\"pt\").to(model.device)\n",
        "outputs = model.generate(**inputs, max_new_tokens=1, use_cache=True)\n",
        "decoded = tokenizer.batch_decode(outputs)\n",
        "print(decoded)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "f422JgM9sdVT"
      },
      "source": [
        "### Saving to float16 for VLLM\n",
        "\n",
        "We also support saving to `float16` directly. Select `merged_16bit` for float16 or `merged_4bit` for int4. We also allow `lora` adapters as a fallback. Use `push_to_hub_merged` to upload to your Hugging Face account! You can go to https://huggingface.co/settings/tokens for your personal tokens."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "iHjt_SMYsd3P"
      },
      "outputs": [],
      "source": [
        "# Merge to 16bit\n",
        "if False: model.save_pretrained_merged(\"hf/model\", tokenizer, save_method = \"merged_16bit\",)\n",
        "if False: model.push_to_hub_merged(\"hf/model\", tokenizer, save_method = \"merged_16bit\", token = \"\")\n",
        "\n",
        "# Merge to 4bit\n",
        "if False: model.save_pretrained_merged(\"hf/model\", tokenizer, save_method = \"merged_4bit\",)\n",
        "if False: model.push_to_hub_merged(\"hf/model\", tokenizer, save_method = \"merged_4bit\", token = \"\")\n",
        "\n",
        "# Just LoRA adapters\n",
        "if False: model.save_pretrained_merged(\"model\", tokenizer, save_method = \"lora\",)\n",
        "if False: model.push_to_hub_merged(\"hf/model\", tokenizer, save_method = \"lora\", token = \"\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TCv4vXHd61i7"
      },
      "source": [
        "### GGUF / llama.cpp Conversion\n",
        "To save to `GGUF` / `llama.cpp`, we support it natively now! We clone `llama.cpp` and we default save it to `q8_0`. We allow all methods like `q4_k_m`. Use `save_pretrained_gguf` for local saving and `push_to_hub_gguf` for uploading to HF."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FqfebeAdT073"
      },
      "outputs": [],
      "source": [
        "# Save to 8bit Q8_0\n",
        "if False: model.save_pretrained_gguf(\"model\", tokenizer,)\n",
        "if False: model.push_to_hub_gguf(\"hf/model\", tokenizer, token = \"\")\n",
        "\n",
        "# Save to 16bit GGUF\n",
        "if False: model.save_pretrained_gguf(\"model\", tokenizer, quantization_method = \"f16\")\n",
        "if False: model.push_to_hub_gguf(\"hf/model\", tokenizer, quantization_method = \"f16\", token = \"\")\n",
        "\n",
        "# Save to q4_k_m GGUF\n",
        "if False: model.save_pretrained_gguf(\"model\", tokenizer, quantization_method = \"q4_k_m\")\n",
        "if False: model.push_to_hub_gguf(\"hf/model\", tokenizer, quantization_method = \"q4_k_m\", token = \"\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bDp0zNpwe6U_"
      },
      "source": [
        "Now, use the `model-unsloth.gguf` file or `model-unsloth-Q4_K_M.gguf` file in `llama.cpp` or a UI based system like `GPT4All`. You can install GPT4All by going [here](https://gpt4all.io/index.html)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Zt9CHJqO6p30"
      },
      "source": [
        "And we're done! If you have any questions on Unsloth, we have a [Discord](https://discord.gg/u54VK8m8tk) channel! If you find any bugs or want to keep updated with the latest LLM stuff, or need help, join projects etc, feel free to join our Discord!\n",
        "\n",
        "Some other links:\n",
        "1. Zephyr DPO 2x faster [free Colab](https://colab.research.google.com/drive/15vttTpzzVXv_tJwEk-hIcQ0S9FcEWvwP?usp=sharing)\n",
        "2. Llama 7b 2x faster [free Colab](https://colab.research.google.com/drive/1lBzz5KeZJKXjvivbYvmGarix9Ao6Wxe5?usp=sharing)\n",
        "3. TinyLlama 4x faster full Alpaca 52K in 1 hour [free Colab](https://colab.research.google.com/drive/1AZghoNBQaMDgWJpi4RbffGM1h6raLUj9?usp=sharing)\n",
        "4. CodeLlama 34b 2x faster [A100 on Colab](https://colab.research.google.com/drive/1y7A0AxE3y8gdj4AVkl2aZX47Xu3P1wJT?usp=sharing)\n",
        "5. Llama 7b [free Kaggle](https://www.kaggle.com/danielhanchen/unsloth-alpaca-t4-ddp)\n",
        "6. We also did a [blog](https://huggingface.co/blog/unsloth-trl) with ðŸ¤— HuggingFace, and we're in the TRL [docs](https://huggingface.co/docs/trl/main/en/sft_trainer#accelerate-fine-tuning-2x-using-unsloth)!\n",
        "\n",
        "<div class=\"align-center\">\n",
        "  <a href=\"https://github.com/unslothai/unsloth\"><img src=\"https://github.com/unslothai/unsloth/raw/main/images/unsloth%20new%20logo.png\" width=\"115\"></a>\n",
        "  <a href=\"https://discord.gg/u54VK8m8tk\"><img src=\"https://github.com/unslothai/unsloth/raw/main/images/Discord.png\" width=\"145\"></a>\n",
        "  <a href=\"https://ko-fi.com/unsloth\"><img src=\"https://github.com/unslothai/unsloth/raw/main/images/Kofi button.png\" width=\"145\"></a></a> Support our work if you can! Thanks!\n",
        "</div>"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kaggle": {
      "accelerator": "gpu",
      "dataSources": [
        {
          "datasetId": 5081962,
          "sourceId": 8512897,
          "sourceType": "datasetVersion"
        }
      ],
      "dockerImageVersionId": 30733,
      "isGpuEnabled": true,
      "isInternetEnabled": true,
      "language": "python",
      "sourceType": "notebook"
    },
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.3"
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "f81dc2d82a3a40bfba2560334ed575f1": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_ce9e82adcb674363b08e59f5d6027a89",
              "IPY_MODEL_757c212d2bb84e4f8124eab80ba6616e",
              "IPY_MODEL_f5598cb64c284d4683edc35e643aa344"
            ],
            "layout": "IPY_MODEL_54aa5cd4d59c4a8a841fe507aecc7e82"
          }
        },
        "ce9e82adcb674363b08e59f5d6027a89": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_94bfc954ff494f94911bd5625f6b0d4b",
            "placeholder": "â€‹",
            "style": "IPY_MODEL_c4fac1f150654605a11a52d939006679",
            "value": "Loadingâ€‡checkpointâ€‡shards:â€‡100%"
          }
        },
        "757c212d2bb84e4f8124eab80ba6616e": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_2519d9d3848c476f8124528ed8b1fec5",
            "max": 2,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_e4982dac8d4642f1a5397067ca8d0e4a",
            "value": 2
          }
        },
        "f5598cb64c284d4683edc35e643aa344": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_31ef391052f2483d8cacbcf4b60b5a56",
            "placeholder": "â€‹",
            "style": "IPY_MODEL_5c98c02b9c964ea0baef7e29ef112a74",
            "value": "â€‡2/2â€‡[00:32&lt;00:00,â€‡15.49s/it]"
          }
        },
        "54aa5cd4d59c4a8a841fe507aecc7e82": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "94bfc954ff494f94911bd5625f6b0d4b": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "c4fac1f150654605a11a52d939006679": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "2519d9d3848c476f8124528ed8b1fec5": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "e4982dac8d4642f1a5397067ca8d0e4a": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "31ef391052f2483d8cacbcf4b60b5a56": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "5c98c02b9c964ea0baef7e29ef112a74": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "73fdc72db4cf48ca91c304fc916c5056": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_edf6cb91e4e24ac3b6444defc64e7084",
              "IPY_MODEL_79255cc3e7e448d88d6e1ed99f9cd215",
              "IPY_MODEL_6131d8150321495db60b6e5ca2c8989b"
            ],
            "layout": "IPY_MODEL_87dec61a36054740b362fedf7af14c84"
          }
        },
        "edf6cb91e4e24ac3b6444defc64e7084": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_18ca03ba99b8483ebaaf9de295858f5f",
            "placeholder": "â€‹",
            "style": "IPY_MODEL_51e62733729744a7a4c31f72579a19ea",
            "value": "Unsloth:â€‡Tokenizingâ€‡[&quot;text&quot;]â€‡(num_proc=6):â€‡100%"
          }
        },
        "79255cc3e7e448d88d6e1ed99f9cd215": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_874f67fd43024451b743fe20d9765b02",
            "max": 16781,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_39ada60923354c3fac8ffb845c2797e5",
            "value": 16781
          }
        },
        "6131d8150321495db60b6e5ca2c8989b": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_c0d254c7633d42c2a9ca886411955538",
            "placeholder": "â€‹",
            "style": "IPY_MODEL_3b6a3bf99bc841fc91e0a6c114e2f3c7",
            "value": "â€‡16781/16781â€‡[00:08&lt;00:00,â€‡5195.22â€‡examples/s]"
          }
        },
        "87dec61a36054740b362fedf7af14c84": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "18ca03ba99b8483ebaaf9de295858f5f": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "51e62733729744a7a4c31f72579a19ea": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "874f67fd43024451b743fe20d9765b02": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "39ada60923354c3fac8ffb845c2797e5": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "c0d254c7633d42c2a9ca886411955538": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "3b6a3bf99bc841fc91e0a6c114e2f3c7": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        }
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}